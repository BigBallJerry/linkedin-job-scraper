{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape the job data of DA from linkedin and analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### libs and environment setup  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the linkedin login page\n",
    "def loginLinkedIn(browser, email, password):\n",
    "  browser.get('https://www.linkedin.com/login')\n",
    "\n",
    "  # wait 4s until all elements are loaded.\n",
    "  time.sleep(4)\n",
    "\n",
    "  # find the element ids with the Chrome inspector\n",
    "  # send in username and password the corresponding input elements\n",
    "  browser.find_element(By.ID, 'username').send_keys(email)\n",
    "  browser.find_element(By.ID, 'password').send_keys(password)\n",
    "\n",
    "  # send in 'submit' event\n",
    "  browser.find_element(By.CLASS_NAME, 'btn__primary--large').click()\n",
    "  \n",
    "  return browser;\n",
    "\n",
    "# scroll window function\n",
    "def scroll(browser, timeout):\n",
    "  # You can set your own pause time. My laptop is a bit slow so I use 1 sec\n",
    "  scroll_pause_time = timeout \n",
    "  # get the screen height of the web\n",
    "  screen_height = browser.execute_script(\"return window.screen.height;\")   \n",
    "  i = 1\n",
    "\n",
    "  while True:\n",
    "    # scroll one screen height each time\n",
    "    browser.execute_script(\"window.scrollTo(0, {screen_height}*{i});\".format(screen_height=screen_height, i=i))  \n",
    "    i += 1\n",
    "    time.sleep(scroll_pause_time)\n",
    "    # update scroll height each time after scrolled, as the scroll height can change after we scrolled the page\n",
    "    scroll_height = browser.execute_script(\"return document.body.scrollHeight;\")  \n",
    "    # Break the loop when the height we need to scroll to is larger than the total scroll height\n",
    "    if (screen_height) * i > scroll_height:\n",
    "        break\n",
    "\n",
    "# Get the job info from the '<li>' element of search results\n",
    "def get_job_info(element):\n",
    "  title = ''\n",
    "  company_name = ''\n",
    "  company_link = ''\n",
    "  location = ''\n",
    "  job_type = ''\n",
    "  job_link = ''\n",
    "\n",
    "  # find the job title\n",
    "  try:\n",
    "    title = element.find_element(By.CLASS_NAME, 'job-card-list__title').text\n",
    "    #print(\"title: \", title)\n",
    "  except:\n",
    "    print(\"title exception occurred\", title)\n",
    "\n",
    "  # find the company name\n",
    "  try:\n",
    "    company_name = element.find_element(By.CLASS_NAME, 'job-card-container__company-name').text\n",
    "    #print(\"company name: \", company_name)\n",
    "  except:\n",
    "    print(\"company_name exception occurred\", company_name)\n",
    "\n",
    "  # find the company link\n",
    "  try:\n",
    "    company_link = element.find_element(By.CLASS_NAME, 'job-card-container__company-name').get_attribute('href')\n",
    "    #print(\"company link: \", company_link)\n",
    "  except:\n",
    "    print(\"company_link exception occurred\", company_link)\n",
    "\n",
    "  # find the location\n",
    "  try:\n",
    "    location = element.find_element(By.CLASS_NAME, 'job-card-container__metadata-item').text.strip()\n",
    "    job_type = element.find_element(By.CLASS_NAME, 'job-card-container__metadata-item--workplace-type').text.strip()\n",
    "    #print(\"location: \", location)\n",
    "    #print(\"job type: \", job_type)\n",
    "  except:\n",
    "    print(\"location exception occurred\", location)\n",
    "\n",
    "  # find the job link\n",
    "  try:\n",
    "    job_link = element.find_element(By.CLASS_NAME, 'job-card-list__title').get_attribute('href')\n",
    "    #print(\"job link: \", job_link)\n",
    "  except:\n",
    "    print(\"job_link exception occurred\", job_link)\n",
    "  \n",
    "  return [['title','company_name','company_link','location','job_type','job_link']]\n",
    "\n",
    "# Get the data from the search results\n",
    "def get_linkedin_job_data(browser, base_url, max_results):\n",
    "  # job data in \n",
    "  job_data = [['title','company_name','company_link','location','job_type','job_link']]\n",
    "  df = pd.DataFrame(job_data)\n",
    "  \n",
    "  job_ids = []\n",
    "  counter = 0\n",
    "\n",
    "  # traverse all required pages\n",
    "  num_pages = int(max_results/25)\n",
    "  for p in range(num_pages):\n",
    "    if counter >= max_results:\n",
    "      break;\n",
    "\n",
    "    # url of each page\n",
    "    url = base_url + '&start=' + str((p+64)*25)\n",
    "    \n",
    "    # get each page\n",
    "    browser.get(url)\n",
    "    time.sleep(4)\n",
    "\n",
    "    # read the job brief from the list\n",
    "    elements = browser.find_elements(By.CLASS_NAME, \"jobs-search-results__list-item\")\n",
    "    print(\"size of elements: \", len(elements))\n",
    "\n",
    "    for e in elements:\n",
    "      counter += 1\n",
    "\n",
    "      if counter >= max_results:\n",
    "        break;\n",
    "\n",
    "      print(counter)\n",
    "\n",
    "      try:\n",
    "        job_id = e.get_attribute(\"data-occludable-job-id\")\n",
    "        job_ids.append(job_id)\n",
    "      except:\n",
    "        print(\"didn't find job id at \", counter)\n",
    "\n",
    "  df = pd.DataFrame(job_ids, columns = ['job_id'])\n",
    "  return df\n",
    "\n",
    "def test_url(browser, url):\n",
    "  browser.get(url)\n",
    "  time.sleep(1)\n",
    "  elements = browser.find_elements(By.CLASS_NAME, \"jobs-search-results__list-item\")\n",
    "  print(\"size of elements: \", len(elements))\n",
    "\n",
    "  counter = 0\n",
    "  for e in elements:\n",
    "    counter += 1\n",
    "    print(counter)\n",
    "    print(e.get_attribute(\"data-occludable-job-id\"))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open the Chrome Browers and login to Linkedin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\58299\\AppData\\Local\\Temp\\ipykernel_40808\\702859995.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(\"./chromedriver.exe\")\n"
     ]
    }
   ],
   "source": [
    "# Create the webdriver instance\n",
    "browser = webdriver.Chrome(\"./chromedriver.exe\")\n",
    "\n",
    "# Credentials\n",
    "email = \"jian@jian.se\"\n",
    "password = \"Jordan23!\"\n",
    "\n",
    "browser = loginLinkedIn(browser, email, password)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape the jobs data\n",
    "Need to search the job title 'Data Analyst' with location 'Stockholm' manually before scraping the search results.\n",
    "After searching manually, we find the layout of the result listed on the left panel for every 25 items per page and there are around 40 pages in total,\n",
    "even though the total search results is 3048 for this search.\n",
    "\n",
    "The URL of the first search is like this.\n",
    "https://www.linkedin.com/jobs/search/?currentJobId=3185920126&geoId=100907646&keywords=data%20analyst&location=Stockholm%2C%20Stockholm%20County%2C%20Sweden&refresh=true\n",
    "\n",
    "When I click on the page \"2\", the URL becomes\n",
    "https://www.linkedin.com/jobs/search/?currentJobId=3185920126&geoId=100907646&keywords=data%20analyst&location=Stockholm%2C%20Stockholm%20County%2C%20Sweden&refresh=true&start=25\n",
    "\n",
    "And for the page \"3\", the URL is\n",
    "https://www.linkedin.com/jobs/search/?currentJobId=3185920126&geoId=100907646&keywords=data%20analyst&location=Stockholm%2C%20Stockholm%20County%2C%20Sweden&refresh=true&start=50\n",
    "\n",
    "So that we can generate the URL of each page n by adding \"&start=25*n\" at the end of base URL.\n",
    "\n",
    "To be able to see the job description of each jobs in the result list, we need to simulate the mouse event to \"Click\" on each item. The Action Chains of selenium can help me with it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of elements:  8\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\workspace\\data-analysis\\linkedin\\scrape-linkedin.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/workspace/data-analysis/linkedin/scrape-linkedin.ipynb#ch0000008?line=1'>2</a>\u001b[0m base_url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://www.linkedin.com/jobs/search/?currentJobId=3185920126&geoId=100907646&keywords=data\u001b[39m\u001b[39m%20a\u001b[39;00m\u001b[39mnalyst&location=Stockholm\u001b[39m\u001b[39m%\u001b[39m\u001b[39m2C\u001b[39m\u001b[39m%\u001b[39m\u001b[39m20Stockholm\u001b[39m\u001b[39m%\u001b[39m\u001b[39m20County\u001b[39m\u001b[39m%\u001b[39m\u001b[39m2C\u001b[39m\u001b[39m%\u001b[39m\u001b[39m20Sweden&refresh=true\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/workspace/data-analysis/linkedin/scrape-linkedin.ipynb#ch0000008?line=3'>4</a>\u001b[0m \u001b[39m# To fetch all pages\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/workspace/data-analysis/linkedin/scrape-linkedin.ipynb#ch0000008?line=4'>5</a>\u001b[0m df \u001b[39m=\u001b[39m get_linkedin_job_data(browser, base_url, \u001b[39m3000\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/workspace/data-analysis/linkedin/scrape-linkedin.ipynb#ch0000008?line=5'>6</a>\u001b[0m df\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mjobs.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/workspace/data-analysis/linkedin/scrape-linkedin.ipynb#ch0000008?line=6'>7</a>\u001b[0m df\u001b[39m.\u001b[39mhead(\u001b[39m10\u001b[39m)\n",
      "\u001b[1;32md:\\workspace\\data-analysis\\linkedin\\scrape-linkedin.ipynb Cell 9\u001b[0m in \u001b[0;36mget_linkedin_job_data\u001b[1;34m(browser, base_url, max_results)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/workspace/data-analysis/linkedin/scrape-linkedin.ipynb#ch0000008?line=102'>103</a>\u001b[0m \u001b[39m# get each page\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/workspace/data-analysis/linkedin/scrape-linkedin.ipynb#ch0000008?line=103'>104</a>\u001b[0m browser\u001b[39m.\u001b[39mget(url)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/workspace/data-analysis/linkedin/scrape-linkedin.ipynb#ch0000008?line=104'>105</a>\u001b[0m time\u001b[39m.\u001b[39;49msleep(\u001b[39m4\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/workspace/data-analysis/linkedin/scrape-linkedin.ipynb#ch0000008?line=106'>107</a>\u001b[0m \u001b[39m# read the job brief from the list\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/workspace/data-analysis/linkedin/scrape-linkedin.ipynb#ch0000008?line=107'>108</a>\u001b[0m elements \u001b[39m=\u001b[39m browser\u001b[39m.\u001b[39mfind_elements(By\u001b[39m.\u001b[39mCLASS_NAME, \u001b[39m\"\u001b[39m\u001b[39mjobs-search-results__list-item\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# To fetch the job data\n",
    "base_url = \"https://www.linkedin.com/jobs/search/?currentJobId=3185920126&geoId=100907646&keywords=data%20analyst&location=Stockholm%2C%20Stockholm%20County%2C%20Sweden&refresh=true\"\n",
    "\n",
    "# To fetch all pages\n",
    "df = get_linkedin_job_data(browser, base_url, 3000)\n",
    "df.to_csv('jobs.csv', index=False)\n",
    "df.head(10)\n",
    "\n",
    "#test_url(browser, \"https://www.linkedin.com/jobs/search/?currentJobId=3185920126&geoId=100907646&keywords=data%20analyst&location=Stockholm%2C%20Stockholm%20County%2C%20Sweden&refresh=true&start=9\")\n",
    "\n",
    "#scroll(browser,2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "033e51eebe5a68942c3e6259377abe06197dfd8ef37d95772ac18a584ac7f4f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
