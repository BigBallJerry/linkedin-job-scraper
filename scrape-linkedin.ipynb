{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape the job data of DA from linkedin and analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### libs and environment setup  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the linkedin login page\n",
    "def loginLinkedIn(browser, email, password):\n",
    "  browser.get('https://www.linkedin.com/login')\n",
    "\n",
    "  # wait 4s until all elements are loaded.\n",
    "  time.sleep(4)\n",
    "\n",
    "  # find the element ids with the Chrome inspector\n",
    "  # send in username and password the corresponding input elements\n",
    "  browser.find_element(By.ID, 'username').send_keys(email)\n",
    "  browser.find_element(By.ID, 'password').send_keys(password)\n",
    "\n",
    "  # send in 'submit' event\n",
    "  browser.find_element(By.CLASS_NAME, 'btn__primary--large').click()\n",
    "  \n",
    "  return browser;\n",
    "\n",
    "# scroll window function\n",
    "def scroll(browser, timeout):\n",
    "  # You can set your own pause time. My laptop is a bit slow so I use 1 sec\n",
    "  scroll_pause_time = timeout \n",
    "  # get the screen height of the web\n",
    "  screen_height = browser.execute_script(\"return window.screen.height;\")   \n",
    "  i = 1\n",
    "\n",
    "  while True:\n",
    "    # scroll one screen height each time\n",
    "    browser.execute_script(\"window.scrollTo(0, {screen_height}*{i});\".format(screen_height=screen_height, i=i))  \n",
    "    i += 1\n",
    "    time.sleep(scroll_pause_time)\n",
    "    # update scroll height each time after scrolled, as the scroll height can change after we scrolled the page\n",
    "    scroll_height = browser.execute_script(\"return document.body.scrollHeight;\")  \n",
    "    # Break the loop when the height we need to scroll to is larger than the total scroll height\n",
    "    if (screen_height) * i > scroll_height:\n",
    "        break\n",
    "\n",
    "# Get the data from the search results\n",
    "def get_linkedin_job_data(browser, base_url, max_results):\n",
    "  job_ids = []\n",
    "  counter = 0\n",
    "\n",
    "  # traverse all required pages\n",
    "  num_pages = int(max_results/25)\n",
    "  for p in range(num_pages):\n",
    "    if counter >= max_results:\n",
    "      break;\n",
    "\n",
    "    # url of each page\n",
    "    url = base_url + '&start=' + str((p+64)*25)\n",
    "    \n",
    "    # get each page\n",
    "    browser.get(url)\n",
    "    time.sleep(4)\n",
    "\n",
    "    # read the job brief from the list\n",
    "    elements = browser.find_elements(By.CLASS_NAME, \"jobs-search-results__list-item\")\n",
    "    print(\"size of elements: \", len(elements))\n",
    "\n",
    "    for e in elements:\n",
    "      counter += 1\n",
    "\n",
    "      if counter >= max_results:\n",
    "        break;\n",
    "\n",
    "      print(counter)\n",
    "\n",
    "      try:\n",
    "        job_id = e.get_attribute(\"data-occludable-job-id\")\n",
    "        job_ids.append(job_id)\n",
    "      except:\n",
    "        print(\"didn't find job id at \", counter)\n",
    "\n",
    "  df = pd.DataFrame(job_ids, columns = ['job_id'])\n",
    "  return df\n",
    "\n",
    "def test_url(browser, url):\n",
    "  browser.get(url)\n",
    "  time.sleep(1)\n",
    "  elements = browser.find_elements(By.CLASS_NAME, \"jobs-search-results__list-item\")\n",
    "  print(\"size of elements: \", len(elements))\n",
    "\n",
    "  counter = 0\n",
    "  for e in elements:\n",
    "    counter += 1\n",
    "    print(counter)\n",
    "    print(e.get_attribute(\"data-occludable-job-id\"))\n",
    "\n",
    "# Get the job info from the '<li>' element of search results\n",
    "def get_job_info(browser, base_url, job_id):\n",
    "  # define all the required fileds\n",
    "  job_title = ''           # Senior Data Analyst\n",
    "  company_name = ''        # Agoda\n",
    "  company_link = ''        # https://www.linkedin.com/company/agoda/life/atagoda/\n",
    "  job_location = ''        # Hybrid, On-site, jobs-unified-top-card__workplace-type\n",
    "  publish_date = ''        # 1 week ago, jobs-unified-top-card__posted-date\n",
    "  applicant_count = ''     # jobs-unified-top-card__applicant-count\n",
    "  job_type = ''            # Full-time\n",
    "  job_level = ''           # Associate, Entry level\n",
    "  company_size = ''        # 5,001-10,000 employees 路 \n",
    "  company_industry = ''    #  路 Technology, Information and Internet\n",
    "  connection_count = ''    # 1 connection, 2 school alumni\n",
    "  connection_type = ''     # connection, alumni\n",
    "  job_description = ''     # content description\n",
    "\n",
    "  # Load the job page\n",
    "  browser.get(base_url + str(job_id))\n",
    "  time.sleep(1)\n",
    "\n",
    "  # find the job title\n",
    "  try:\n",
    "    job_title = browser.find_element(By.TAG_NAME, 'h1').text\n",
    "    #print(\"title: \", job_title)\n",
    "  except:\n",
    "    print(\"job_title exception occurred\", job_title, job_id)\n",
    "\n",
    "  # find the company name\n",
    "  try:\n",
    "    company_name_element = browser.find_element(By.CLASS_NAME, 'jobs-unified-top-card__company-name').find_element(By.TAG_NAME, 'a')\n",
    "    company_name = company_name_element.text\n",
    "    company_link = company_name_element.get_attribute('href')\n",
    "  except:\n",
    "    print(\"company_name exception occurred\", company_name, job_id)\n",
    "\n",
    "  # find the publish_date\n",
    "  try:\n",
    "    publish_date = browser.find_element(By.CLASS_NAME, 'jobs-unified-top-card__posted-date').text\n",
    "    #print(\"publish date: \", publish_date)\n",
    "  except:\n",
    "    print(\"publish_date exception occurred\", publish_date, job_id)\n",
    "\n",
    "  # find the applicant_count\n",
    "  try:\n",
    "    #jobs-unified-top-card__applicant-count jobs-unified-top-card__applicant-count--low t-bold\n",
    "    applicant_count = browser.find_element(By.CLASS_NAME, 'jobs-unified-top-card__applicant-count').text.split(' ',1)[0]\n",
    "    #print(\"publish date: \", applicant_count)\n",
    "  except:\n",
    "    print(\"applicant_count exception occurred\", applicant_count, job_id)\n",
    "\n",
    "  # find all the job insights\n",
    "  try:\n",
    "    job_insights = browser.find_elements(By.CLASS_NAME, 'jobs-unified-top-card__job-insight')\n",
    "\n",
    "    for insight in job_insights:\n",
    "      try:\n",
    "        insight_type = insight.find_element(By.TAG_NAME, 'li-icon').get_attribute('type')\n",
    "        insight_content = insight.find_element(By.TAG_NAME, 'span').text\n",
    "      except:\n",
    "        print(\"insight_type exception occurred\", insight_type, job_id)\n",
    "\n",
    "      if insight_type == \"job\":\n",
    "        contents = insight_content.strip().split('路')\n",
    "        job_type = contents[0].strip()\n",
    "        job_level = contents[1].strip()\n",
    "      elif insight_type == \"company-icon\":\n",
    "        contents = insight_content.strip().split('路')\n",
    "        company_size = contents[0].split(' ')[0].strip()\n",
    "        company_industry = contents[1].strip()\n",
    "      elif insight_type == \"people\":\n",
    "        # 1 connection / 2 school alumni\n",
    "        contents = insight_content.strip().split(' ', 1)\n",
    "        connection_count = contents[0].strip()\n",
    "        connection_type = contents[1].strip()\n",
    "      # else:\n",
    "      #   print(\"insight_type not supported\", job_id)\n",
    "\n",
    "  except:\n",
    "    print(\"job_insights exception occurred\", applicant_count, job_id)\n",
    "\n",
    "\n",
    "  # find the job_location\n",
    "  try:\n",
    "    job_location = browser.find_element(By.CLASS_NAME, 'jobs-unified-top-card__workplace-type').text.strip()\n",
    "    #print(\"location: \", job_location)\n",
    "  except:\n",
    "    print(\"job_location exception occurred\", job_location, job_id)\n",
    "\n",
    "  # find the job description\n",
    "  try:\n",
    "    job_description = browser.find_element(By.CLASS_NAME, 'jobs-box__html-content').text.strip()\n",
    "    job_description = job_description.replace('\\n',' ')\n",
    "    #print(\"job_description: \", job_description)\n",
    "  except:\n",
    "    print(\"job_description exception occurred\", job_description, job_id)\n",
    "  \n",
    "  return [job_id, job_title, company_name, company_link, job_location, publish_date, applicant_count, job_type, job_level, company_size, company_industry, connection_count, connection_type, job_description]\n",
    "\n",
    "def get_job_page_data(browser, job_id_list_file):\n",
    "  # job data list\n",
    "  job_data = []\n",
    "\n",
    "  # read ids from csv file saved in previous step\n",
    "  df = pd.read_csv(job_id_list_file)\n",
    "  job_id_list = df['job_id'].drop_duplicates().values.tolist()\n",
    "\n",
    "  # fetch all the job pages by job_id\n",
    "  base_url = \"https://www.linkedin.com/jobs/view/\"\n",
    "  \n",
    "  for id in job_id_list:\n",
    "    job_data.append(get_job_info(browser, base_url, id))\n",
    "\n",
    "  df2 = pd.DataFrame(job_data, \n",
    "    columns =['job_id', 'job_title', 'company_name', 'company_link', 'job_location', 'publish_date', 'applicant_count', 'job_type', 'job_level', 'company_size', 'company_industry', 'connection_count', 'connection_type', 'job_description']) \n",
    "\n",
    "  return df2\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open the Chrome Browers and login to Linkedin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eorcdeg\\AppData\\Local\\Temp\\ipykernel_23468\\702859995.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(\"./chromedriver.exe\")\n"
     ]
    }
   ],
   "source": [
    "# Create the webdriver instance\n",
    "browser = webdriver.Chrome(\"./chromedriver.exe\")\n",
    "\n",
    "# Credentials\n",
    "email = \"jian@jian.se\"\n",
    "password = \"Jordan23!\"\n",
    "\n",
    "browser = loginLinkedIn(browser, email, password)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape the jobs data\n",
    "Need to search the job title 'Data Analyst' with location 'Stockholm' manually before scraping the search results.\n",
    "After searching manually, we find the layout of the result listed on the left panel for every 25 items per page and there are around 40 pages in total,\n",
    "even though the total search results is 3048 for this search.\n",
    "\n",
    "The URL of the first search is like this.\n",
    "https://www.linkedin.com/jobs/search/?currentJobId=3185920126&geoId=100907646&keywords=data%20analyst&location=Stockholm%2C%20Stockholm%20County%2C%20Sweden&refresh=true\n",
    "\n",
    "When I click on the page \"2\", the URL becomes\n",
    "https://www.linkedin.com/jobs/search/?currentJobId=3185920126&geoId=100907646&keywords=data%20analyst&location=Stockholm%2C%20Stockholm%20County%2C%20Sweden&refresh=true&start=25\n",
    "\n",
    "And for the page \"3\", the URL is\n",
    "https://www.linkedin.com/jobs/search/?currentJobId=3185920126&geoId=100907646&keywords=data%20analyst&location=Stockholm%2C%20Stockholm%20County%2C%20Sweden&refresh=true&start=50\n",
    "\n",
    "So that we can generate the URL of each page n by adding \"&start=25*n\" at the end of base URL.\n",
    "\n",
    "To be able to see the job description of each jobs in the result list, we need to simulate the mouse event to \"Click\" on each item. The Action Chains of selenium can help me with it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of elements:  8\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\workspace\\dotjs\\linkedin-job-scraper\\scrape-linkedin.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/workspace/dotjs/linkedin-job-scraper/scrape-linkedin.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m base_url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://www.linkedin.com/jobs/search/?currentJobId=3185920126&geoId=100907646&keywords=data\u001b[39m\u001b[39m%20a\u001b[39;00m\u001b[39mnalyst&location=Stockholm\u001b[39m\u001b[39m%\u001b[39m\u001b[39m2C\u001b[39m\u001b[39m%\u001b[39m\u001b[39m20Stockholm\u001b[39m\u001b[39m%\u001b[39m\u001b[39m20County\u001b[39m\u001b[39m%\u001b[39m\u001b[39m2C\u001b[39m\u001b[39m%\u001b[39m\u001b[39m20Sweden&refresh=true\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/workspace/dotjs/linkedin-job-scraper/scrape-linkedin.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# To fetch all pages\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/workspace/dotjs/linkedin-job-scraper/scrape-linkedin.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m df \u001b[39m=\u001b[39m get_linkedin_job_data(browser, base_url, \u001b[39m3000\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/workspace/dotjs/linkedin-job-scraper/scrape-linkedin.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m df\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mjobs.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/workspace/dotjs/linkedin-job-scraper/scrape-linkedin.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m df\u001b[39m.\u001b[39mhead(\u001b[39m10\u001b[39m)\n",
      "\u001b[1;32mc:\\workspace\\dotjs\\linkedin-job-scraper\\scrape-linkedin.ipynb Cell 9\u001b[0m in \u001b[0;36mget_linkedin_job_data\u001b[1;34m(browser, base_url, max_results)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/workspace/dotjs/linkedin-job-scraper/scrape-linkedin.ipynb#X11sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m url \u001b[39m=\u001b[39m base_url \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m&start=\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m((p\u001b[39m+\u001b[39m\u001b[39m64\u001b[39m)\u001b[39m*\u001b[39m\u001b[39m25\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/workspace/dotjs/linkedin-job-scraper/scrape-linkedin.ipynb#X11sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# get each page\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/workspace/dotjs/linkedin-job-scraper/scrape-linkedin.ipynb#X11sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m browser\u001b[39m.\u001b[39;49mget(url)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/workspace/dotjs/linkedin-job-scraper/scrape-linkedin.ipynb#X11sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m4\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/workspace/dotjs/linkedin-job-scraper/scrape-linkedin.ipynb#X11sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m# read the job brief from the list\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\eorcdeg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:440\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, url: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    437\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[39m    Loads a web page in the current browser session.\u001b[39;00m\n\u001b[0;32m    439\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 440\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mGET, {\u001b[39m'\u001b[39;49m\u001b[39murl\u001b[39;49m\u001b[39m'\u001b[39;49m: url})\n",
      "File \u001b[1;32mc:\\Users\\eorcdeg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:426\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    423\u001b[0m         params[\u001b[39m'\u001b[39m\u001b[39msessionId\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession_id\n\u001b[0;32m    425\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_value(params)\n\u001b[1;32m--> 426\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcommand_executor\u001b[39m.\u001b[39;49mexecute(driver_command, params)\n\u001b[0;32m    427\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[0;32m    428\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merror_handler\u001b[39m.\u001b[39mcheck_response(response)\n",
      "File \u001b[1;32mc:\\Users\\eorcdeg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:344\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    342\u001b[0m data \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mdump_json(params)\n\u001b[0;32m    343\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_url\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 344\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(command_info[\u001b[39m0\u001b[39;49m], url, body\u001b[39m=\u001b[39;49mdata)\n",
      "File \u001b[1;32mc:\\Users\\eorcdeg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:366\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    363\u001b[0m     body \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_alive:\n\u001b[1;32m--> 366\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conn\u001b[39m.\u001b[39;49mrequest(method, url, body\u001b[39m=\u001b[39;49mbody, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[0;32m    367\u001b[0m     statuscode \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus\n\u001b[0;32m    368\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\eorcdeg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\request.py:78\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[1;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_encode_url(\n\u001b[0;32m     75\u001b[0m         method, url, fields\u001b[39m=\u001b[39mfields, headers\u001b[39m=\u001b[39mheaders, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39murlopen_kw\n\u001b[0;32m     76\u001b[0m     )\n\u001b[0;32m     77\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_encode_body(\n\u001b[0;32m     79\u001b[0m         method, url, fields\u001b[39m=\u001b[39mfields, headers\u001b[39m=\u001b[39mheaders, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39murlopen_kw\n\u001b[0;32m     80\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\eorcdeg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\request.py:170\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m extra_kw[\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mupdate(headers)\n\u001b[0;32m    168\u001b[0m extra_kw\u001b[39m.\u001b[39mupdate(urlopen_kw)\n\u001b[1;32m--> 170\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murlopen(method, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mextra_kw)\n",
      "File \u001b[1;32mc:\\Users\\eorcdeg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\poolmanager.py:376\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    374\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(method, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    375\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 376\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(method, u\u001b[39m.\u001b[39mrequest_uri, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    378\u001b[0m redirect_location \u001b[39m=\u001b[39m redirect \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mget_redirect_location()\n\u001b[0;32m    379\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[1;32mc:\\Users\\eorcdeg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    704\u001b[0m     conn,\n\u001b[0;32m    705\u001b[0m     method,\n\u001b[0;32m    706\u001b[0m     url,\n\u001b[0;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    711\u001b[0m )\n\u001b[0;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[0;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\eorcdeg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[0;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\eorcdeg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[0;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\eorcdeg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py:1371\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1369\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1370\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1371\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1372\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[0;32m   1373\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\eorcdeg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py:319\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 319\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    320\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[0;32m    321\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\eorcdeg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py:280\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 280\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    281\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[0;32m    282\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\eorcdeg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# To fetch the job data\n",
    "base_url = \"https://www.linkedin.com/jobs/search/?currentJobId=3185920126&geoId=100907646&keywords=data%20analyst&location=Stockholm%2C%20Stockholm%20County%2C%20Sweden&refresh=true\"\n",
    "\n",
    "# To fetch all pages\n",
    "df = get_linkedin_job_data(browser, base_url, 3000)\n",
    "df.to_csv('jobs.csv', index=False)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "985\n"
     ]
    }
   ],
   "source": [
    "# fetch the job data by job ids\n",
    "df_jobs = get_job_page_data(browser, 'jobs.copy.csv')\n",
    "\n",
    "#df_jobs.head(10)\n",
    "\n",
    "df_jobs.to_csv('job-data.csv', index=False)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b901e172c3f1d173c97bf43c501639e66d25c84b5b96cb9fc81b96471c07e2ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
